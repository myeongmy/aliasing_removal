{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "data_path = './cropped_train/'\n",
    "data_path_tgt = './cropped_tgt_train/'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=data_path, transform=data_transform)\n",
    "train_dataset_tgt = torchvision.datasets.ImageFolder(root=data_path_tgt, transform=data_transform)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(train_dataset_tgt))\n",
    "\n",
    "data_path1 = './warped_test/'\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=data_path1, transform=data_transform)\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = torch.utils.data.TensorDataset(train_dataset, train_dataset_tgt)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "train_loader_tgt = torch.utils.data.DataLoader(dataset=train_dataset_tgt,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"Custom U-Net architecture for Noise2Noise (see Appendix, Table 2).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        \"\"\"Initializes U-Net.\"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Layers: enc_conv0, enc_conv1, pool1\n",
    "        self._block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 48, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(48, 48, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        # Layers: enc_conv(i), pool(i); i=2..5\n",
    "        self._block2 = nn.Sequential(\n",
    "            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        # Layers: enc_conv6, upsample5\n",
    "        self._block3 = nn.Sequential(\n",
    "            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(48, 48, 3, stride=2, padding=1, output_padding=1))\n",
    "            #nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "\n",
    "        # Layers: dec_conv5a, dec_conv5b, upsample4\n",
    "        self._block4 = nn.Sequential(\n",
    "            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=0, output_padding=0))\n",
    "            \n",
    "            #nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "\n",
    "        # Layers: dec_deconv(i)a, dec_deconv(i)b, upsample(i-1); i=4..2\n",
    "        self._block5 = nn.Sequential(\n",
    "            nn.Conv2d(144, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=0, output_padding=0))\n",
    "            #nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "\n",
    "        # Layers: dec_conv1a, dec_conv1b, dec_conv1c,\n",
    "        self._block6 = nn.Sequential(\n",
    "            nn.Conv2d(96 + in_channels, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, out_channels, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1))\n",
    "        \n",
    "        \n",
    "        self._block7 = nn.Sequential(\n",
    "            nn.Conv2d(144, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=1, output_padding=1))\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initializes weights using He et al. (2015).\"\"\"\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Through encoder, then decoder by adding U-skip connections. \"\"\"\n",
    "\n",
    "        # Encoder\n",
    "        pool1 = self._block1(x)\n",
    "        pool2 = self._block2(pool1)\n",
    "        pool3 = self._block2(pool2)\n",
    "        #print('Encoder 3', pool3.shape)\n",
    "        pool4 = self._block2(pool3)\n",
    "        #print('Encoder 4', pool4.shape)\n",
    "        pool5 = self._block2(pool4)\n",
    "        #print('Encoder 5', pool5.shape)\n",
    "\n",
    "        # Decoder\n",
    "        upsample5 = self._block3(pool5)\n",
    "        #print('upsample5', upsample5.shape)\n",
    "        concat5 = torch.cat((upsample5, pool4), dim=1)\n",
    "        #print('Decoder 5', concat5.shape)\n",
    "        upsample4 = self._block4(concat5)\n",
    "        concat4 = torch.cat((upsample4, pool3), dim=1)\n",
    "        #print('Decoder 4', concat4.shape)\n",
    "        upsample3 = self._block5(concat4)\n",
    "        concat3 = torch.cat((upsample3, pool2), dim=1)\n",
    "        #print('Decoder 3', concat3.shape)\n",
    "        upsample2 = self._block7(concat3)\n",
    "        concat2 = torch.cat((upsample2, pool1), dim=1)\n",
    "        #print('Decoder 2', concat2.shape)\n",
    "        upsample1 = self._block7(concat2)\n",
    "        concat1 = torch.cat((upsample1, x), dim=1)\n",
    "\n",
    "        # Final activation\n",
    "        return self._block6(concat1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss: 0.000005295641\n",
      "Epoch [1/5], Step [200/1875], Loss: 0.000001288189\n",
      "Epoch [1/5], Step [300/1875], Loss: 0.000002138699\n",
      "Epoch [1/5], Step [400/1875], Loss: 0.000000562943\n",
      "Epoch [1/5], Step [500/1875], Loss: 0.000000437425\n",
      "Epoch [1/5], Step [600/1875], Loss: 0.000000374227\n",
      "Epoch [1/5], Step [700/1875], Loss: 0.000000155231\n",
      "Epoch [1/5], Step [800/1875], Loss: 0.000000052106\n",
      "Epoch [1/5], Step [900/1875], Loss: 0.000000068431\n",
      "Epoch [1/5], Step [1000/1875], Loss: 0.000000012280\n",
      "Epoch [1/5], Step [1100/1875], Loss: 0.000000011497\n",
      "Epoch [1/5], Step [1200/1875], Loss: 0.000000003072\n",
      "Epoch [1/5], Step [1300/1875], Loss: 0.000000004613\n",
      "Epoch [1/5], Step [1400/1875], Loss: 0.000000007298\n",
      "Epoch [1/5], Step [1500/1875], Loss: 0.000000004751\n",
      "Epoch [1/5], Step [1600/1875], Loss: 0.000000004061\n",
      "Epoch [1/5], Step [1700/1875], Loss: 0.000000008096\n",
      "Epoch [1/5], Step [1800/1875], Loss: 0.000000002294\n",
      "Epoch [2/5], Step [100/1875], Loss: 0.000000000672\n",
      "Epoch [2/5], Step [200/1875], Loss: 0.000000001043\n",
      "Epoch [2/5], Step [300/1875], Loss: 0.000000003262\n",
      "Epoch [2/5], Step [400/1875], Loss: 0.000000000801\n",
      "Epoch [2/5], Step [500/1875], Loss: 0.000000001304\n",
      "Epoch [2/5], Step [600/1875], Loss: 0.000000000781\n",
      "Epoch [2/5], Step [700/1875], Loss: 0.000000001220\n",
      "Epoch [2/5], Step [800/1875], Loss: 0.000000001252\n",
      "Epoch [2/5], Step [900/1875], Loss: 0.000000002889\n",
      "Epoch [2/5], Step [1000/1875], Loss: 0.000000000424\n",
      "Epoch [2/5], Step [1100/1875], Loss: 0.000000000707\n",
      "Epoch [2/5], Step [1200/1875], Loss: 0.000000000383\n",
      "Epoch [2/5], Step [1300/1875], Loss: 0.000000000405\n",
      "Epoch [2/5], Step [1400/1875], Loss: 0.000000001793\n",
      "Epoch [2/5], Step [1500/1875], Loss: 0.000000000770\n",
      "Epoch [2/5], Step [1600/1875], Loss: 0.000000000397\n",
      "Epoch [2/5], Step [1700/1875], Loss: 0.000000001348\n",
      "Epoch [2/5], Step [1800/1875], Loss: 0.000000000722\n",
      "Epoch [3/5], Step [100/1875], Loss: 0.000000000163\n",
      "Epoch [3/5], Step [200/1875], Loss: 0.000000000240\n",
      "Epoch [3/5], Step [300/1875], Loss: 0.000000000552\n",
      "Epoch [3/5], Step [400/1875], Loss: 0.000000000193\n",
      "Epoch [3/5], Step [500/1875], Loss: 0.000000000373\n",
      "Epoch [3/5], Step [600/1875], Loss: 0.000000000306\n",
      "Epoch [3/5], Step [700/1875], Loss: 0.000000000306\n",
      "Epoch [3/5], Step [800/1875], Loss: 0.000000000538\n",
      "Epoch [3/5], Step [900/1875], Loss: 0.000000001129\n",
      "Epoch [3/5], Step [1000/1875], Loss: 0.000000000174\n",
      "Epoch [3/5], Step [1100/1875], Loss: 0.000000000212\n",
      "Epoch [3/5], Step [1200/1875], Loss: 0.000000000100\n",
      "Epoch [3/5], Step [1300/1875], Loss: 0.000000000117\n",
      "Epoch [3/5], Step [1400/1875], Loss: 0.000000000284\n",
      "Epoch [3/5], Step [1500/1875], Loss: 0.000000000378\n",
      "Epoch [3/5], Step [1600/1875], Loss: 0.000000000124\n",
      "Epoch [3/5], Step [1700/1875], Loss: 0.000000000417\n",
      "Epoch [3/5], Step [1800/1875], Loss: 0.000000000175\n",
      "Epoch [4/5], Step [100/1875], Loss: 0.000000000071\n",
      "Epoch [4/5], Step [200/1875], Loss: 0.000000000092\n",
      "Epoch [4/5], Step [300/1875], Loss: 0.000000000483\n",
      "Epoch [4/5], Step [400/1875], Loss: 0.000000000084\n",
      "Epoch [4/5], Step [500/1875], Loss: 0.000000000164\n",
      "Epoch [4/5], Step [600/1875], Loss: 0.000000000086\n",
      "Epoch [4/5], Step [700/1875], Loss: 0.000000000198\n",
      "Epoch [4/5], Step [800/1875], Loss: 0.000000000158\n",
      "Epoch [4/5], Step [900/1875], Loss: 0.000000000269\n",
      "Epoch [4/5], Step [1000/1875], Loss: 0.000000000050\n",
      "Epoch [4/5], Step [1100/1875], Loss: 0.000000000130\n",
      "Epoch [4/5], Step [1200/1875], Loss: 0.000000000044\n",
      "Epoch [4/5], Step [1300/1875], Loss: 0.000000000088\n",
      "Epoch [4/5], Step [1400/1875], Loss: 0.000000000221\n",
      "Epoch [4/5], Step [1500/1875], Loss: 0.000000000297\n",
      "Epoch [4/5], Step [1600/1875], Loss: 0.000000000060\n",
      "Epoch [4/5], Step [1700/1875], Loss: 0.000000000194\n",
      "Epoch [4/5], Step [1800/1875], Loss: 0.000000000070\n",
      "Epoch [5/5], Step [100/1875], Loss: 0.000000000028\n",
      "Epoch [5/5], Step [200/1875], Loss: 0.000000000034\n",
      "Epoch [5/5], Step [300/1875], Loss: 0.000000000167\n",
      "Epoch [5/5], Step [400/1875], Loss: 0.000000000044\n",
      "Epoch [5/5], Step [500/1875], Loss: 0.000000000048\n",
      "Epoch [5/5], Step [600/1875], Loss: 0.000000000037\n",
      "Epoch [5/5], Step [700/1875], Loss: 0.000000000074\n",
      "Epoch [5/5], Step [800/1875], Loss: 0.000000000048\n",
      "Epoch [5/5], Step [900/1875], Loss: 0.000000000045\n",
      "Epoch [5/5], Step [1000/1875], Loss: 0.000000000021\n",
      "Epoch [5/5], Step [1100/1875], Loss: 0.000000000050\n",
      "Epoch [5/5], Step [1200/1875], Loss: 0.000000000017\n",
      "Epoch [5/5], Step [1300/1875], Loss: 0.000000000029\n",
      "Epoch [5/5], Step [1400/1875], Loss: 0.000000000060\n",
      "Epoch [5/5], Step [1500/1875], Loss: 0.000000000103\n",
      "Epoch [5/5], Step [1600/1875], Loss: 0.000000000014\n",
      "Epoch [5/5], Step [1700/1875], Loss: 0.000000000084\n",
      "Epoch [5/5], Step [1800/1875], Loss: 0.000000000029\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(zip(train_loader, train_loader_tgt)):\n",
    "        images = data[0][0].to(device)\n",
    "        images_tgt = data[1][0].to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs_tgt = model(images_tgt)\n",
    "\n",
    "        loss = criterion(outputs, outputs_tgt)\n",
    "    \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.12f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1512, 2016])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 252 and 253 in dimension 3 at /tmp/pip-req-build-58y_cjjl/aten/src/THC/generic/THCTensorMath.cu:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6147b1305e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-467f4771b0b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m#print('Decoder 5', concat5.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mupsample4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mconcat4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsample4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;31m#print('Decoder 4', concat4.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mupsample3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 252 and 253 in dimension 3 at /tmp/pip-req-build-58y_cjjl/aten/src/THC/generic/THCTensorMath.cu:71"
     ]
    }
   ],
   "source": [
    "new_model =UNet().to(device)\n",
    "new_model.load_state_dict(torch.load(\"model.ckpt\"))\n",
    "\n",
    "\n",
    "\n",
    "# Test the model\n",
    "new_model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        print(images.shape)\n",
    "        outputs = new_model(images)\n",
    "        print(outputs.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
